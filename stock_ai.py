# -*- coding: utf-8 -*-
"""Trading4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SKqkUJeJpH4wH-ftWsZAgRCpGl6_pHfu
"""
import pandas as pd
import numpy as np
import tensorflow as tf
import pandas as pd
from keras.models import load_model
from csv_parser import write_data, request_stocks
import csv
import datetime
from learning_ai import read_data


def slice_to_sections(data, x_sections_cnt, y_sections_cnt):
  sections_cnt = x_sections_cnt + y_sections_cnt
  section_count = len(data) // (sections_cnt)
  xs = []
  for i in range(x_sections_cnt):
    xs.append(data[i::sections_cnt])
  ys = []
  for i in range(x_sections_cnt, x_sections_cnt + y_sections_cnt):
    ys.append(data[i::sections_cnt])
  min_section_size = min(min([len(i) for i in xs]), min(len(i) for i in ys))
  for i in range(x_sections_cnt):
    xs[i] = xs[i][:min_section_size]
  for i in range(y_sections_cnt):
    ys[i] = ys[i][:min_section_size]
  return xs, ys


def unification(xs, ys):
  y = np.array(ys).T
  x = np.array(xs).T
  return x, y


def pack(m1, m2):
  return np.column_stack((m1, m2))


def unpack(h):
  # print("h", h)
  # print("out", np.hsplit(h, 2))
  return np.hsplit(h, 2)


def grounding(xs, ys, s=1):
  mnx = np.min(xs, axis=1)[:, np.newaxis]
  mxx = np.max(xs, axis=1)[:, np.newaxis]
  x = (xs - mnx) / (mxx - mnx)
  y = (ys - mnx) / (mxx - mnx)
  h = pack(mnx, mxx)
  return x, y, h


def grounding_one(xs, s=1):
  mnx = np.min(xs, axis=1)[:, np.newaxis]
  mxx = np.max(xs, axis=1)[:, np.newaxis]
  x = (xs - mnx) / (mxx - mnx)
  h = pack(mnx, mxx)
  return x, h


def ungrounding(x, y, mxx, s=1):
  mnx, mxx = unpack(mxx)
  xs = x * (mxx - mnx) + mnx
  ys = y * (mxx - mnx) + mnx
  return xs, ys

def ungrounding_one(x, mxx, s=1):
  mnx, mxx = unpack(mxx)
  xs = x * (mxx - mnx) + mnx
  return xs


def generate(data, x_sections_cnt, y_sections_cnt):
  gx, gy, gmxx = [], [], []
  for swp in range(x_sections_cnt):
    x, y = slice_to_sections(data[swp:], x_sections_cnt, y_sections_cnt)
    x, y = unification(x, y)
    x, y, mxx = grounding(x, y, 1)
    gx.extend(x)
    gy.extend(y)
    # print(gy)
    gmxx.extend(mxx)
  print(len(gx), len(gy), len(gmxx))
  minlen = min(len(gx), len(gy), len(gmxx))
  print(minlen)
  gx = gx[:minlen]
  gy = gy[:minlen]
  gmxx = gmxx[:minlen]
  return np.array(gx), np.array(gy), np.array(gmxx)


def make_selections(x, y, pl, mxx):
  pt = 100 - pl
  indicesl = np.random.permutation(len(x))
  x = x[indicesl]
  y = y[indicesl]
  mxx = mxx[indicesl]

  xs = min(len(x), round((len(x) * pl) // 100))
  xl = x[:xs]
  mxxl = mxx[:xs]
  xt = x[xs:]
  mxxt = mxx[xs:]

  ys = min(len(y), round((len(y) * pl) // 100))
  yl = y[:ys]
  yt = y[ys:]

  return xl, xt, yl, yt, mxxl, mxxt


def main():
  with open("all.csv") as f:
    companies = [e[1] for e in csv.reader(f, delimiter=";")]
  companies = companies
  for c in companies:
    print(f"Downloadding data {c}")
    write_data(request_stocks(datetime.datetime(2000, 1, 1), c), "models/" + c + ".csv")
    print(f"Predictng {c}")
    data = read_data(f"models/{c}.csv", delimiter=';')
    data = np.array(data[["Open", "Close"]].mean(axis=1))
    print(len(data))
    x, y, mxx = generate(data, 100, 5)
    print(len(x), len(y), len(mxx))
    # loading
    model = load_model(f"models/{c}_model.h5")
    p = model.predict(x)
    print(p)


if __name__ == '__main__':
  main()